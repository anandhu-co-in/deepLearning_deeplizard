{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we will create a simple sequential model, train it, predict using it and also plot the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels =  []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example data: \n",
    "- An experiemental drug was tested on individuals from ages 13 to 100. \n",
    "- The trial had 2100 participants. Half were under 65 years old, half were over 65 years old.\n",
    "- 95% of patientes 65 or older experienced side effects.\n",
    "- 95% of patients under 65 experienced no side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    # The 5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    # The 95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    # The 95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 84, 53, 68, 13, 65, 55, 76, 26, 87, 56, 99, 33, 75, 38, 83, 14, 92, 28, 79, 25, 93, 40, 81, 34, 73, 26, 93, 58, 66, 27, 79, 30, 74, 35, 92, 18, 87, 60, 69, 45, 92, 31, 76, 49, 93, 34, 81, 43, 93, 58, 73, 21, 69, 27, 84, 27, 78, 48, 86, 43, 87, 62, 83, 24, 77, 30, 98, 53, 90, 62, 98, 59, 98, 23, 95, 49, 95, 25, 65, 17, 79, 42, 74, 27, 84, 42, 94, 32, 71, 64, 91, 43, 66, 53, 83, 40, 84, 28, 88, 60, 65, 58, 76, 60, 83, 20, 98, 63, 66, 14, 87, 33, 74, 29, 82, 39, 86, 55, 88, 20, 70, 18, 67, 39, 77, 47, 89, 27, 88, 13, 78, 46, 74, 22, 80, 56, 71, 59, 65, 36, 67, 29, 98, 13, 84, 31, 78, 20, 72, 21, 90, 40, 81, 44, 70, 47, 76, 55, 96, 17, 85, 51, 65, 49, 82, 33, 95, 33, 71, 33, 88, 53, 97, 43, 83, 21, 68, 15, 99, 22, 86, 34, 65, 47, 73, 24, 69, 19, 98, 23, 78, 17, 89, 58, 93, 17, 67, 60, 87, 25, 66, 22, 74, 38, 86, 48, 71, 17, 74, 41, 91, 46, 97, 42, 69, 57, 86, 58, 94, 59, 89, 24, 73, 29, 95, 51, 100, 29, 86, 25, 68, 31, 96, 46, 73, 40, 97, 63, 80, 53, 91, 46, 80, 32, 87, 21, 71, 56, 91, 21, 79, 52, 67, 35, 87, 30, 86, 53, 84, 40, 70, 44, 88, 58, 85, 15, 66, 39, 70, 18, 91, 62, 75, 43, 80, 32, 75, 13, 90, 20, 94, 58, 96, 16, 66, 36, 87, 37, 75, 20, 71, 43, 90, 54, 68, 23, 71, 16, 86, 37, 70, 32, 90, 17, 70, 28, 85, 19, 89, 28, 81, 22, 86, 18, 85, 14, 90, 34, 76, 53, 66, 31, 97, 34, 79, 29, 84, 54, 70, 26, 74, 13, 81, 45, 93, 13, 95, 55, 93, 46, 71, 52, 83, 59, 97, 64, 91, 64, 78, 45, 72, 26, 70, 22, 79, 13, 93, 64, 76, 47, 94, 26, 96, 18, 83, 44, 92, 42, 67, 57, 83, 41, 72, 30, 81, 24, 83, 45, 92, 49, 97, 52, 84, 26, 74, 52, 86, 64, 70, 25, 78, 54, 77, 57, 85, 33, 65, 41, 80, 44, 99, 50, 94, 54, 85, 21, 77, 37, 86, 41, 88, 51, 87, 22, 74, 35, 89, 37, 85, 25, 100, 40, 93, 38, 95, 30, 91, 45, 98, 24, 94, 37, 91, 26, 94, 48, 90, 15, 76, 13, 94, 36, 97, 40, 99, 60, 86, 22, 84, 61, 68, 42, 93, 33, 92, 30, 88, 19, 93, 59, 99, 51, 67, 24, 89, 61, 93, 35, 79, 37, 91, 30, 100, 48, 76, 51, 70, 39, 76, 46, 85, 49, 90, 13, 100, 50, 88, 27, 86, 32, 95, 32, 91, 26, 94, 61, 99, 18, 80, 52, 97, 59, 69, 14, 73, 58, 80, 58, 99, 44, 77, 41, 82, 53, 91, 34, 92, 25, 93, 45, 72, 36, 78, 18, 68, 36, 80, 64, 95, 46, 99, 42, 94, 39, 76, 32, 100, 59, 71, 41, 66, 32, 69, 63, 100, 57, 81, 55, 72, 13, 94, 62, 88, 26, 90, 61, 84, 35, 66, 59, 92, 18, 72, 45, 71, 45, 84, 14, 81, 28, 72, 36, 72, 55, 77, 47, 87, 21, 85, 55, 71, 17, 88, 34, 81, 18, 70, 46, 84, 45, 68, 64, 78, 47, 85, 31, 71, 26, 83, 55, 99, 58, 90, 59, 83, 60, 90, 26, 71, 48, 75, 13, 65, 57, 85, 50, 91, 53, 92, 24, 87, 21, 87, 29, 71, 21, 85, 13, 80, 63, 94, 61, 81, 15, 78, 60, 91, 57, 86, 14, 89, 47, 73, 16, 84, 54, 76, 60, 94, 27, 93, 36, 73, 44, 70, 59, 92, 14, 94, 38, 72, 54, 75, 19, 93, 31, 91, 44, 67, 33, 91, 36, 96, 32, 74, 54, 71, 59, 80, 41, 73, 19, 84, 46, 100, 26, 93, 17, 78, 36, 89, 22, 67, 22, 77, 23, 94, 45, 79, 38, 83, 22, 94, 61, 86, 18, 72, 35, 72, 52, 83, 31, 68, 45, 86, 58, 91, 43, 95, 19, 89, 36, 100, 17, 77, 19, 69, 15, 90, 26, 87, 15, 91, 45, 88, 48, 97, 31, 70, 26, 66, 28, 82, 24, 84, 47, 71, 17, 69, 60, 82, 47, 99, 53, 78, 49, 68, 46, 82, 55, 76, 38, 100, 19, 67, 28, 99, 37, 88, 25, 80, 53, 68, 18, 86, 54, 94, 32, 70, 64, 84, 14, 89, 54, 85, 17, 95, 51, 86, 44, 100, 13, 92, 28, 70, 34, 80, 16, 71, 31, 77, 35, 93, 50, 69, 53, 78, 48, 80, 22, 81, 34, 79, 59, 84, 40, 74, 34, 96, 35, 91, 64, 79, 37, 99, 61, 67, 64, 91, 57, 88, 19, 83, 55, 73, 20, 96, 35, 67, 50, 73, 39, 69, 32, 72, 47, 85, 35, 89, 13, 100, 21, 100, 55, 89, 18, 91, 29, 76, 14, 75, 56, 78, 34, 77, 55, 87, 42, 94, 14, 78, 13, 89, 31, 99, 55, 87, 25, 68, 42, 99, 36, 79, 24, 77, 48, 68, 42, 75, 43, 83, 43, 93, 57, 100, 23, 79, 61, 93, 59, 66, 42, 75, 53, 78, 22, 70, 58, 72, 59, 90, 52, 79, 50, 69, 36, 87, 28, 91, 38, 75, 30, 70, 39, 83, 59, 83, 55, 90, 49, 82, 57, 79, 58, 99, 41, 94, 43, 67, 37, 93, 56, 88, 19, 81, 34, 71, 25, 88, 40, 86, 47, 89, 23, 91, 44, 94, 43, 99, 36, 93, 44, 95, 40, 71, 59, 72, 55, 72, 56, 96, 23, 77, 24, 91, 29, 94, 54, 80, 55, 69, 18, 79, 36, 70, 30, 65, 63, 82, 59, 90, 46, 95, 21, 98, 54, 65, 28, 88, 59, 86, 46, 73, 18, 83, 18, 97, 16, 93, 23, 87, 29, 98, 59, 73, 21, 88, 22, 81, 30, 91, 22, 94, 37, 77, 39, 94, 48, 91, 53, 79, 33, 84, 48, 76, 58, 85, 50, 83, 44, 79, 29, 72, 26, 90, 23, 72, 36, 72, 57, 76, 22, 67, 46, 83, 39, 88, 55, 71, 28, 69, 19, 69, 47, 92, 63, 67, 25, 66, 57, 87, 31, 82, 61, 86, 21, 67, 15, 80, 42, 89, 57, 98, 26, 72, 63, 86, 47, 98, 24, 80, 50, 73, 57, 72, 62, 80, 18, 97, 15, 77, 53, 93, 23, 99, 22, 77, 42, 99, 56, 80, 45, 81, 22, 81, 58, 67, 56, 73, 57, 78, 50, 94, 28, 99, 24, 67, 56, 87, 41, 97, 30, 73, 55, 75, 25, 97, 57, 69, 26, 94, 27, 68, 28, 94, 51, 84, 16, 76, 42, 96, 31, 65, 30, 71, 55, 73, 47, 92, 40, 71, 34, 86, 41, 81, 32, 96, 24, 77, 45, 79, 59, 97, 40, 82, 44, 92, 30, 95, 22, 89, 39, 85, 59, 74, 22, 95, 43, 80, 27, 75, 56, 73, 24, 99, 26, 97, 61, 77, 40, 97, 54, 84, 53, 74, 31, 96, 24, 73, 28, 95, 53, 74, 32, 69, 28, 88, 36, 73, 23, 73, 29, 68, 37, 88, 38, 77, 45, 66, 19, 76, 56, 74, 39, 98, 23, 70, 49, 97, 33, 97, 48, 77, 32, 90, 20, 94, 41, 99, 35, 82, 32, 80, 61, 90, 28, 76, 34, 80, 24, 75, 35, 69, 57, 91, 42, 84, 31, 90, 22, 84, 37, 68, 50, 85, 34, 100, 20, 70, 16, 92, 51, 81, 57, 75, 61, 96, 46, 85, 41, 80, 28, 85, 34, 87, 47, 78, 31, 83, 63, 84, 32, 94, 56, 82, 62, 91, 62, 80, 54, 99, 13, 66, 45, 96, 14, 71, 32, 93, 21, 72, 27, 97, 34, 76, 57, 82, 22, 94, 62, 95, 46, 100, 38, 80, 14, 85, 48, 66, 49, 100, 35, 87, 28, 92, 49, 74, 62, 84, 64, 82, 52, 71, 44, 68, 31, 78, 19, 91, 43, 81, 15, 65, 43, 74, 58, 95, 16, 75, 43, 93, 14, 78, 17, 89, 57, 92, 30, 99, 63, 83, 37, 97, 25, 91, 18, 89, 31, 76, 31, 94, 43, 100, 31, 99, 44, 96, 44, 91, 58, 90, 55, 83, 42, 88, 13, 80, 55, 88, 56, 96, 54, 77, 31, 85, 20, 90, 42, 77, 21, 88, 42, 89, 33, 91, 34, 91, 35, 81, 17, 94, 22, 68, 34, 72, 33, 95, 46, 77, 18, 100, 42, 82, 50, 74, 34, 91, 31, 70, 42, 70, 18, 87, 14, 88, 41, 86, 47, 68, 58, 73, 38, 75, 46, 95, 22, 89, 23, 90, 31, 72, 30, 76, 50, 78, 47, 71, 33, 65, 27, 85, 60, 69, 56, 82, 52, 94, 50, 98, 48, 92, 63, 90, 50, 75, 47, 96, 53, 74, 15, 66, 28, 88, 29, 83, 52, 71, 21, 92, 21, 70, 27, 75, 26, 98, 38, 94, 43, 91, 39, 70, 25, 89, 47, 86, 39, 68, 39, 73, 25, 69, 15, 74, 14, 67, 17, 80, 40, 88, 38, 74, 26, 78, 42, 68, 61, 98, 44, 97, 53, 83, 24, 98, 29, 95, 57, 74, 50, 78, 32, 92, 27, 67, 59, 98, 49, 91, 59, 92, 24, 92, 13, 100, 42, 99, 24, 89, 13, 83, 31, 91, 36, 95, 21, 73, 50, 76, 63, 92, 43, 81, 16, 67, 34, 91, 49, 91, 33, 66, 50, 97, 13, 88, 43, 75, 34, 70, 57, 68, 52, 70, 41, 85, 20, 66, 14, 88, 41, 100, 61, 85, 40, 94, 35, 68, 22, 99, 17, 66, 13, 85, 27, 73, 46, 86, 13, 93, 58, 70, 23, 96, 43, 91, 47, 76, 23, 88, 37, 87, 30, 100, 33, 92, 20, 93, 50, 97, 32, 77, 32, 73, 27, 65, 34, 77, 59, 92, 19, 75, 58, 80, 27, 72, 29, 68, 47, 83, 44, 71, 56, 88, 16, 85, 33, 70, 31, 72, 15, 87, 52, 100, 62, 95, 30, 72, 62, 71, 53, 84, 58, 94, 26, 94, 25, 78, 41, 90, 28, 99, 36, 94, 51, 77, 45, 67, 63, 90, 33, 88, 39, 77, 60, 99, 53, 82, 50, 78, 16, 66, 46, 100, 13, 66, 32, 68, 32, 78, 33, 85, 29, 68, 24, 96, 28, 95, 64, 79, 58, 96, 53, 78, 45, 72, 37, 74, 52, 99, 30, 85, 17, 70, 39, 91, 28, 69, 49, 80, 58, 88, 38, 77, 26, 81, 32, 86, 53, 80, 48, 98, 21, 81, 57, 68, 17, 80, 51, 91, 33, 83, 60, 84, 14, 98, 27, 92, 48, 69, 53, 98, 25, 88, 26, 72, 35, 67, 27, 72, 48, 72, 25, 72, 51, 78, 56, 92, 32, 79, 14, 70, 46, 70, 44, 95, 17, 71, 40, 77, 44, 83, 51, 78, 20, 80, 44, 81, 57, 85, 37, 100, 20, 79, 29, 92, 60, 87, 24, 88, 39, 74, 44, 93, 38, 74, 63, 65, 26, 82, 25, 96, 32, 73, 52, 85, 47, 89, 33, 100, 34, 98, 52, 69, 58, 81, 42, 73, 30, 67, 25, 76, 32, 78, 29, 97, 26, 66, 28, 93, 60, 68, 60, 73, 58, 75, 19, 78, 52, 66, 62, 87, 16, 66, 63, 65, 13, 95, 20, 95, 62, 72, 13, 84, 27, 88, 19, 92, 64, 99, 58, 77, 47, 87, 31, 88, 36, 75, 41, 84, 47, 86, 62, 82, 47, 73, 21, 76, 55, 79, 34, 73, 27, 90, 19, 90, 31, 75, 55, 85, 53, 100, 14, 90, 34, 81, 25, 98, 57, 83, 23, 69, 16, 99, 37, 80, 15, 99, 56, 98, 42, 96, 29, 98, 56, 70, 59, 90, 50, 65, 28, 82, 26, 66, 60, 73, 44, 83, 37, 76, 43, 90, 58, 68, 42, 74, 36, 72, 64, 80, 55, 79, 48, 66, 61, 68, 24, 94, 48, 91, 20, 74, 29, 82, 17, 98, 48, 86, 19, 94, 53, 77, 21, 73, 41, 78, 45, 89, 31, 78, 62, 81, 25, 83, 42, 96, 63, 66, 15, 73, 38, 79, 27, 93, 36, 67, 26, 80, 28, 67, 27, 84, 29, 91, 33, 78, 16, 82, 46, 85, 55, 87, 29, 70, 25, 87, 14, 80, 30, 100, 14, 65, 63, 70, 30, 71, 31, 84, 54, 83, 25, 72, 61, 66, 55, 72, 37, 81, 60, 85, 31, 84, 44, 69, 27, 83, 54, 79, 26, 100, 15, 77]\n"
     ]
    }
   ],
   "source": [
    "print(train_samples)\n",
    "# for i in train_samples:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)\n",
    "# for i in train_labels:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25  84  53 ... 100  15  77]\n",
      "[1 0 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert both lists into numpy arrays \n",
    "# https://stackoverflow.com/questions/993984/what-are-the-advantages-of-numpy-over-regular-python-lists\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "#train_labels,train_samples=shuffle(train_labels,train_samples)\n",
    "print(train_samples)\n",
    "print(train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 5 6]\n",
      "(5,)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [5]\n",
      " [6]]\n",
      "(5, 1)\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[5]\n",
      "[6]\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Reshaping tutorial\n",
    "abc = np.array([1,2,3,5,6])\n",
    "print(abc)\n",
    "print(abc.shape)\n",
    "print(abc.reshape(-1,1))\n",
    "print(abc.reshape(-1,1).shape)\n",
    "\n",
    "for i in abc.reshape(-1,1):\n",
    "    print(i)\n",
    "    \n",
    "for i in abc:\n",
    "    print(i)\n",
    "    \n",
    "#Syntax\n",
    "#reshape(blocks,rows,columns), or reshape(rows,columns)\n",
    "#reshape(-1,1) : -1 to create as many rows as in the original array, 1 means i need 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13793103]\n",
      " [0.81609195]\n",
      " [0.45977011]\n",
      " ...\n",
      " [1.        ]\n",
      " [0.02298851]\n",
      " [0.73563218]]\n"
     ]
    }
   ],
   "source": [
    "#use sclar object to reshape fit transofrm the input vals to 0-1 form\n",
    "#reshape because the function acceepts 2Dimenationsl input\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform((train_samples).reshape(-1,1))  \n",
    "print(scaled_train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Device Code here, refer 21:36 in https://www.youtube.com/watch?v=qFJeN9V1ZsI&t=3990s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(1,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#relu activation function -  output the input directly if it is positive, otherwise, it will output zero\n",
    "#softmax - Specifically, the network is configured to output N values, one for each class in the classification task, and the softmax function is used to normalize the outputs, converting them from weighted sum values into probabilities that sum to one. Each value in the output of the softmax function is interpreted as the probability of membership for each class.\n",
    "#shape (1,) -> Because the shapey of input eg [0.03448276] is (1,)\n",
    "#Softmax  -> Outputs probabily distributions, which when sumed is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Adam -Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data\n",
    "# sparse_categorical_crossentropy - The lose function that we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13793103]\n",
      " [0.81609195]\n",
      " [0.45977011]\n",
      " ...\n",
      " [1.        ]\n",
      " [0.02298851]\n",
      " [0.73563218]]\n",
      "[1 0 1 ... 1 0 1]\n",
      "Epoch 1/20\n",
      "189/189 - 0s - loss: 0.6942 - accuracy: 0.5503 - val_loss: 0.6799 - val_accuracy: 0.7238\n",
      "Epoch 2/20\n",
      "189/189 - 0s - loss: 0.6679 - accuracy: 0.7127 - val_loss: 0.6508 - val_accuracy: 0.7714\n",
      "Epoch 3/20\n",
      "189/189 - 0s - loss: 0.6334 - accuracy: 0.7889 - val_loss: 0.6017 - val_accuracy: 0.8524\n",
      "Epoch 4/20\n",
      "189/189 - 0s - loss: 0.5859 - accuracy: 0.8656 - val_loss: 0.5531 - val_accuracy: 0.9000\n",
      "Epoch 5/20\n",
      "189/189 - 0s - loss: 0.5463 - accuracy: 0.8794 - val_loss: 0.5118 - val_accuracy: 0.9095\n",
      "Epoch 6/20\n",
      "189/189 - 0s - loss: 0.5104 - accuracy: 0.8847 - val_loss: 0.4725 - val_accuracy: 0.9095\n",
      "Epoch 7/20\n",
      "189/189 - 0s - loss: 0.4773 - accuracy: 0.8852 - val_loss: 0.4364 - val_accuracy: 0.9143\n",
      "Epoch 8/20\n",
      "189/189 - 0s - loss: 0.4471 - accuracy: 0.8968 - val_loss: 0.4035 - val_accuracy: 0.9333\n",
      "Epoch 9/20\n",
      "189/189 - 0s - loss: 0.4198 - accuracy: 0.9164 - val_loss: 0.3713 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "189/189 - 0s - loss: 0.3961 - accuracy: 0.9042 - val_loss: 0.3439 - val_accuracy: 0.9286\n",
      "Epoch 11/20\n",
      "189/189 - 0s - loss: 0.3754 - accuracy: 0.9138 - val_loss: 0.3202 - val_accuracy: 0.9333\n",
      "Epoch 12/20\n",
      "189/189 - 0s - loss: 0.3580 - accuracy: 0.9185 - val_loss: 0.2988 - val_accuracy: 0.9333\n",
      "Epoch 13/20\n",
      "189/189 - 0s - loss: 0.3434 - accuracy: 0.9196 - val_loss: 0.2808 - val_accuracy: 0.9476\n",
      "Epoch 14/20\n",
      "189/189 - 0s - loss: 0.3312 - accuracy: 0.9233 - val_loss: 0.2644 - val_accuracy: 0.9476\n",
      "Epoch 15/20\n",
      "189/189 - 0s - loss: 0.3212 - accuracy: 0.9275 - val_loss: 0.2507 - val_accuracy: 0.9476\n",
      "Epoch 16/20\n",
      "189/189 - 0s - loss: 0.3131 - accuracy: 0.9265 - val_loss: 0.2390 - val_accuracy: 0.9476\n",
      "Epoch 17/20\n",
      "189/189 - 0s - loss: 0.3063 - accuracy: 0.9254 - val_loss: 0.2292 - val_accuracy: 0.9571\n",
      "Epoch 18/20\n",
      "189/189 - 0s - loss: 0.3007 - accuracy: 0.9286 - val_loss: 0.2202 - val_accuracy: 0.9571\n",
      "Epoch 19/20\n",
      "189/189 - 0s - loss: 0.2962 - accuracy: 0.9302 - val_loss: 0.2124 - val_accuracy: 0.9571\n",
      "Epoch 20/20\n",
      "189/189 - 0s - loss: 0.2924 - accuracy: 0.9312 - val_loss: 0.2057 - val_accuracy: 0.9571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d90ad7f520>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scaled_train_samples) # note that this is 2d\n",
    "print(train_labels) # and this is 1d\n",
    "\n",
    "# model.fit(scaled_train_samples, train_labels, batch_size=10, epochs=20, shuffle=True, verbose=2)\n",
    "\n",
    "#Method 2 - with validatin set. Use a specofid split just to validate, It wont be used to train (note that here the validation split it taken before the shuffle)\n",
    "model.fit(scaled_train_samples, train_labels,validation_split=0.1, batch_size=10, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('medical_trial_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This save functions saves:\n",
    "- The architecture of the model, allowing to re-create the model.\n",
    "- The weights of the model.\n",
    "- The training configuration (loss, optimizer).\n",
    "- The state of the optimizer, allowing to resume training exactly where you left off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('medical_trial_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.54927546, -0.4856965 ,  0.53571576,  0.4868819 , -0.398502  ,\n",
       "          0.5780775 , -0.27317312, -0.25490513, -0.2938473 ,  0.6408101 ,\n",
       "          0.07305707, -0.32130396,  0.31824487, -0.32005355, -0.05071509,\n",
       "          0.15298308]], dtype=float32),\n",
       " array([-0.13936894,  0.        , -0.07624116, -0.11410188,  0.        ,\n",
       "        -0.11703212,  0.        ,  0.        ,  0.        , -0.0843427 ,\n",
       "         0.20109741,  0.        , -0.02657733,  0.        ,  0.        ,\n",
       "         0.1840929 ], dtype=float32),\n",
       " array([[-0.30210087, -0.1306758 , -0.35159373, -0.08438969,  0.35406896,\n",
       "         -0.35605603, -0.41543278,  0.32360098, -0.16107935,  0.24767384,\n",
       "          0.52726424,  0.19752073, -0.28003785, -0.03598413,  0.10331412,\n",
       "          0.10215285, -0.29636577, -0.00634175, -0.33093178, -0.02407849,\n",
       "         -0.3910657 , -0.4151323 , -0.24281104, -0.17975332,  0.18296656,\n",
       "          0.21664524, -0.3192843 , -0.17306557,  0.17942533, -0.3597774 ,\n",
       "         -0.313846  ,  0.11945418],\n",
       "        [ 0.02226642,  0.0877054 ,  0.2785978 ,  0.2958078 , -0.24475375,\n",
       "         -0.1631968 ,  0.16054866,  0.23730287, -0.05751419, -0.3455776 ,\n",
       "         -0.06494382, -0.31934127, -0.23769532, -0.28352118, -0.23189795,\n",
       "         -0.35288587, -0.27960834, -0.25571293,  0.22578302, -0.3405178 ,\n",
       "         -0.29852605,  0.28216174,  0.01483873,  0.2883818 , -0.1676052 ,\n",
       "          0.19097885,  0.2432374 ,  0.33900896,  0.3476744 ,  0.19352004,\n",
       "          0.19539407, -0.00253978],\n",
       "        [-0.23679334,  0.30715504,  0.3087134 ,  0.01986113,  0.04053562,\n",
       "          0.21090455, -0.10971504, -0.339943  , -0.18748713,  0.185455  ,\n",
       "          0.17242914,  0.10946356,  0.03284444, -0.07217882, -0.3178882 ,\n",
       "          0.09088236, -0.17160207, -0.15649727,  0.21148092, -0.07886475,\n",
       "         -0.31111124, -0.06840373, -0.23977709, -0.2840158 ,  0.4007875 ,\n",
       "          0.14814976, -0.12239122, -0.20977767,  0.49918687,  0.07257895,\n",
       "          0.22235742,  0.07724308],\n",
       "        [-0.13771   ,  0.46790797,  0.1304541 , -0.27031368,  0.1362029 ,\n",
       "         -0.42997018, -0.04915854,  0.2566025 , -0.13419817,  0.10474047,\n",
       "          0.5629588 , -0.1374181 , -0.29021525, -0.28960997, -0.15317063,\n",
       "          0.2175831 ,  0.02835277, -0.03035912,  0.13560158,  0.22711268,\n",
       "          0.03961468, -0.16432722, -0.2776425 , -0.25200102, -0.08881511,\n",
       "          0.44049668, -0.18972564,  0.11450052,  0.26389146, -0.18620588,\n",
       "         -0.29976845, -0.23737977],\n",
       "        [ 0.1170032 , -0.33569974, -0.0758692 , -0.25277892, -0.31591263,\n",
       "         -0.31285107, -0.19790027, -0.12950622,  0.21792427, -0.19297862,\n",
       "         -0.20231372,  0.25694504, -0.04756221, -0.3117615 ,  0.29822597,\n",
       "         -0.15955827,  0.2552099 ,  0.12725997, -0.01146168,  0.20248154,\n",
       "          0.3396428 ,  0.25909665, -0.00228688, -0.17781915,  0.03143615,\n",
       "          0.24059138, -0.09702307,  0.3456131 , -0.2585255 ,  0.2285907 ,\n",
       "          0.25677046, -0.09701833],\n",
       "        [ 0.09298773,  0.47576475, -0.34355593, -0.391592  ,  0.60250163,\n",
       "          0.14895333, -0.27465028, -0.02616754, -0.19596732, -0.2059429 ,\n",
       "          0.13037272,  0.41200477,  0.00369717, -0.31299707, -0.41041052,\n",
       "          0.48334637, -0.3105924 , -0.23855633,  0.16593906, -0.34817442,\n",
       "         -0.25285587, -0.29804853, -0.36560655, -0.37183955,  0.20359825,\n",
       "          0.37693724,  0.1782845 ,  0.23067448, -0.08989426,  0.1056159 ,\n",
       "          0.150923  , -0.139661  ],\n",
       "        [ 0.25433108,  0.25436017,  0.25724432,  0.20427993, -0.35329333,\n",
       "         -0.30568165, -0.15645045, -0.340147  ,  0.0327175 , -0.34593374,\n",
       "         -0.10756938, -0.28335932,  0.3024576 , -0.16252288,  0.12796828,\n",
       "         -0.03679866, -0.3041299 , -0.19159797,  0.2749332 ,  0.08873969,\n",
       "          0.09996516,  0.21113506,  0.2074168 , -0.21912955,  0.26172212,\n",
       "         -0.1914599 ,  0.25603142, -0.01288673, -0.10016933,  0.07550263,\n",
       "         -0.2874105 ,  0.31765357],\n",
       "        [-0.17255543, -0.10859413,  0.15822801, -0.11062022,  0.0030399 ,\n",
       "         -0.17085043,  0.13046142,  0.06893808,  0.06886855,  0.1153217 ,\n",
       "         -0.06995946, -0.15569745, -0.29366988, -0.12830049,  0.14711472,\n",
       "          0.31708202,  0.15621486,  0.00565249, -0.1909978 ,  0.2809004 ,\n",
       "         -0.1572369 ,  0.2590858 ,  0.04485023,  0.20560363, -0.28090587,\n",
       "         -0.3382482 , -0.31663603, -0.08301225,  0.06222072, -0.0692009 ,\n",
       "          0.27103558,  0.30117562],\n",
       "        [ 0.2954518 , -0.17759797,  0.04092324, -0.3422835 , -0.16005231,\n",
       "         -0.26432574,  0.12212554, -0.06711438,  0.04936492, -0.2539748 ,\n",
       "          0.1536152 , -0.02710944, -0.11545749, -0.00808039, -0.0495362 ,\n",
       "         -0.01164609,  0.12143663,  0.03210393, -0.3054629 , -0.20733054,\n",
       "         -0.18227938,  0.08616838, -0.23918125,  0.12532467, -0.14211757,\n",
       "         -0.34153488,  0.15362343, -0.03050074,  0.22624633, -0.18924256,\n",
       "         -0.09059381,  0.08413354],\n",
       "        [ 0.02777051,  0.4509769 ,  0.16352114, -0.2819724 ,  0.3966438 ,\n",
       "          0.13789442, -0.2392618 , -0.18513821,  0.24057503, -0.2542699 ,\n",
       "          0.03252336,  0.3333268 , -0.2719532 , -0.35580394, -0.2261477 ,\n",
       "          0.06916812, -0.3534406 , -0.08414976, -0.29493463, -0.21790156,\n",
       "         -0.26364112,  0.12044262,  0.17965463,  0.2285127 ,  0.3432724 ,\n",
       "          0.2828256 ,  0.06303307, -0.17156313,  0.38904753,  0.21090472,\n",
       "         -0.297005  ,  0.16386351],\n",
       "        [ 0.17832878,  0.12725568,  0.09501904,  0.15475279,  0.0897212 ,\n",
       "          0.31049594, -0.12521276, -0.24704476,  0.13947956, -0.06840795,\n",
       "          0.12225062, -0.15865603, -0.18556513,  0.01862278,  0.18967737,\n",
       "         -0.06652223,  0.10677692,  0.00941915,  0.03712936, -0.0638926 ,\n",
       "          0.49940434,  0.34103587,  0.3576785 ,  0.3105977 ,  0.20553413,\n",
       "          0.30846697,  0.3323718 , -0.2878665 ,  0.05838471, -0.06775542,\n",
       "         -0.0241305 , -0.236486  ],\n",
       "        [ 0.28051397,  0.20425138, -0.12564085,  0.33912274, -0.00723821,\n",
       "          0.10743105, -0.2304652 ,  0.04655489,  0.26943263, -0.32335642,\n",
       "         -0.16281058,  0.2433398 , -0.31531474,  0.08422002,  0.10493299,\n",
       "         -0.2553128 , -0.03945702, -0.14037816,  0.2413998 , -0.28727022,\n",
       "         -0.20892614,  0.1093055 , -0.3423961 ,  0.1980749 , -0.2715136 ,\n",
       "          0.02213132, -0.23758516,  0.22506937,  0.27438638,  0.05282888,\n",
       "         -0.22567892,  0.2601706 ],\n",
       "        [-0.37953055,  0.11272309, -0.3023888 ,  0.0400872 , -0.06104235,\n",
       "         -0.31520998,  0.3730287 ,  0.06569943, -0.27920422, -0.1409335 ,\n",
       "          0.42223817,  0.12020799,  0.2400139 , -0.23840001,  0.08601981,\n",
       "          0.17261894,  0.1615152 , -0.06612481, -0.3973694 ,  0.01136592,\n",
       "          0.3176094 , -0.1472287 ,  0.13527676, -0.25142738,  0.50181985,\n",
       "          0.24715924,  0.04429142, -0.3263438 ,  0.4197194 , -0.39863527,\n",
       "         -0.29657534,  0.16238965],\n",
       "        [-0.32234675,  0.17646405,  0.24418864,  0.21623686, -0.277661  ,\n",
       "          0.03799361, -0.06704214, -0.20321828,  0.26425526, -0.1244953 ,\n",
       "          0.3107461 , -0.25322416, -0.34721264, -0.22631568,  0.19927981,\n",
       "         -0.22050497,  0.27457127,  0.31881478, -0.12330996, -0.03118286,\n",
       "          0.27763054, -0.08964583, -0.15913899,  0.3097665 ,  0.04398343,\n",
       "          0.1471152 ,  0.273051  , -0.03795668,  0.0559271 , -0.33759043,\n",
       "         -0.08272111,  0.22962412],\n",
       "        [ 0.11991149, -0.07170653, -0.25454867, -0.17891067,  0.04404321,\n",
       "         -0.33651686, -0.22783878,  0.28062502,  0.33704975,  0.30946037,\n",
       "         -0.11500281, -0.14971513,  0.01522419,  0.0328806 ,  0.15347853,\n",
       "          0.29208872, -0.26407444, -0.20765263,  0.2084724 , -0.08989552,\n",
       "         -0.13279165, -0.05373514, -0.1587826 , -0.05601871, -0.27487516,\n",
       "         -0.2780629 , -0.30112946,  0.02935442, -0.04997832,  0.1431081 ,\n",
       "          0.22779056,  0.26754668],\n",
       "        [ 0.12839092, -0.05437767, -0.34051168,  0.4332114 ,  0.14101358,\n",
       "         -0.1797859 ,  0.47107247, -0.29455715, -0.08311196, -0.24833378,\n",
       "          0.31365025, -0.12984845,  0.2830758 ,  0.05539208,  0.442584  ,\n",
       "          0.39974415, -0.2633765 ,  0.32075784,  0.19962172, -0.0177671 ,\n",
       "          0.19885613,  0.17728077,  0.07840025,  0.18904993, -0.23710318,\n",
       "          0.06361184, -0.34739226, -0.12144244,  0.28124666, -0.09672464,\n",
       "          0.25323513, -0.07865872]], dtype=float32),\n",
       " array([ 0.17549647, -0.02239842, -0.00059686,  0.18087837, -0.04697077,\n",
       "         0.09051321,  0.18765886,  0.        , -0.01321754,  0.        ,\n",
       "        -0.05184288,  0.00458626,  0.17962119, -0.01482088,  0.16913879,\n",
       "        -0.05126635,  0.        ,  0.10235933,  0.07925625,  0.        ,\n",
       "         0.17530772,  0.1501125 ,  0.14720827,  0.18509492, -0.02041567,\n",
       "        -0.05355729, -0.00399687,  0.        , -0.04625961,  0.14186938,\n",
       "         0.17602593, -0.00515961], dtype=float32),\n",
       " array([[ 0.41624677, -0.4145756 ],\n",
       "        [-0.5477952 ,  0.19699623],\n",
       "        [ 0.17456365,  0.22094274],\n",
       "        [ 0.6646325 , -0.68328565],\n",
       "        [-0.14387308,  0.29940143],\n",
       "        [-0.0894938 , -0.45625234],\n",
       "        [ 0.5721138 , -0.5066662 ],\n",
       "        [ 0.33819363,  0.15889004],\n",
       "        [-0.25318938, -0.11911058],\n",
       "        [-0.4159518 ,  0.00091714],\n",
       "        [-0.43147418,  0.0491874 ],\n",
       "        [-0.18346494,  0.100382  ],\n",
       "        [ 0.1999081 , -0.71984935],\n",
       "        [-0.06930312,  0.08716934],\n",
       "        [ 0.5595008 , -0.6784828 ],\n",
       "        [-0.11522812,  0.45940694],\n",
       "        [ 0.3572258 , -0.35272864],\n",
       "        [ 0.22408853, -0.39442787],\n",
       "        [ 0.4543793 , -0.22868828],\n",
       "        [-0.28388792,  0.11477789],\n",
       "        [ 0.6171331 , -0.53408766],\n",
       "        [ 0.655368  , -0.5389653 ],\n",
       "        [ 0.31652787, -0.5062883 ],\n",
       "        [ 0.5013993 , -0.4222272 ],\n",
       "        [-0.32558066,  0.4305325 ],\n",
       "        [-0.31116548,  0.15899245],\n",
       "        [-0.01669005,  0.2869801 ],\n",
       "        [-0.38441944,  0.18135068],\n",
       "        [-0.19081429,  0.4888433 ],\n",
       "        [ 0.42617676, -0.4127508 ],\n",
       "        [ 0.5316708 , -0.5044403 ],\n",
       "        [ 0.32708028, -0.20808457]], dtype=float32),\n",
       " array([ 0.03976556, -0.03976556], dtype=float32)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x1d90c11d400>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only need to save the architecture of a model, and not its weights or its training configuration, you can use the following function to save the architecture only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as JSON\n",
    "json_string = model.to_json()\n",
    "\n",
    "# save as YAML\n",
    "# yaml_string = model.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_4\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_12_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_12\", \"trainable\": true, \"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_13\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_14\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 2, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.4.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model reconstruction from JSON:\n",
    "# from keras.models import model_from_json\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)\n",
    "\n",
    "# model reconstruction from YAML\n",
    "# from keras.models import model_from_yaml\n",
    "# model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_architecture.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. model.save_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only need to save the weights of a model, you can use the following function save the weights only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(16, input_shape=(1,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels =  []\n",
    "test_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    # The 5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The 95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    # The 95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_test_samples = scaler.fit_transform((test_samples).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 2)\n",
      "[[0.66462773 0.3353723 ]\n",
      " [0.39503235 0.60496765]\n",
      " [0.9309053  0.06909474]\n",
      " [0.1770592  0.82294077]\n",
      " [0.936903   0.06309696]\n",
      " [0.07301755 0.92698246]\n",
      " [0.59416395 0.40583608]\n",
      " [0.14953493 0.85046506]\n",
      " [0.766494   0.2335061 ]\n",
      " [0.05767478 0.9423252 ]\n",
      " [0.95027006 0.0497299 ]\n",
      " [0.39503235 0.60496765]\n",
      " [0.5447184  0.45528156]\n",
      " [0.39503235 0.60496765]\n",
      " [0.830943   0.16905698]\n",
      " [0.12563619 0.87436384]\n",
      " [0.9536411  0.04635891]\n",
      " [0.22554843 0.77445155]\n",
      " [0.9584744  0.04152564]\n",
      " [0.08964885 0.91035116]\n",
      " [0.95841986 0.04158022]\n",
      " [0.11510891 0.8848911 ]\n",
      " [0.49437502 0.50562495]\n",
      " [0.10540269 0.89459735]\n",
      " [0.72844857 0.27155143]\n",
      " [0.06752574 0.9324743 ]\n",
      " [0.9550508  0.04494919]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.8162882  0.18371187]\n",
      " [0.14953493 0.85046506]\n",
      " [0.68673784 0.3132621 ]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.85743886 0.14256117]\n",
      " [0.34795046 0.65204954]\n",
      " [0.95218945 0.0478106 ]\n",
      " [0.34795046 0.65204954]\n",
      " [0.95852894 0.04147113]\n",
      " [0.2827542  0.71724576]\n",
      " [0.95658135 0.04341865]\n",
      " [0.20840824 0.79159176]\n",
      " [0.72844857 0.27155143]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.95841986 0.04158022]\n",
      " [0.07301755 0.92698246]\n",
      " [0.9584744  0.04152564]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.85743886 0.14256117]\n",
      " [0.19224727 0.8077527 ]\n",
      " [0.8006676  0.19933239]\n",
      " [0.10540269 0.89459735]\n",
      " [0.9584744  0.04152564]\n",
      " [0.34795046 0.65204954]\n",
      " [0.95754975 0.04245029]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.9570681  0.04293185]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.9309053  0.06909474]\n",
      " [0.32541698 0.67458296]\n",
      " [0.95827836 0.04172162]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.95827836 0.04172162]\n",
      " [0.04918518 0.9508149 ]\n",
      " [0.95658135 0.04341865]\n",
      " [0.05767478 0.9423252 ]\n",
      " [0.7840702  0.21592979]\n",
      " [0.41938943 0.58061063]\n",
      " [0.6182513  0.38174874]\n",
      " [0.08202943 0.9179706 ]\n",
      " [0.95682544 0.0431746 ]\n",
      " [0.0649257  0.9350743 ]\n",
      " [0.94473624 0.05526375]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.56961465 0.43038532]\n",
      " [0.04918518 0.9508149 ]\n",
      " [0.94473624 0.05526375]\n",
      " [0.1770592  0.82294077]\n",
      " [0.95841986 0.04158022]\n",
      " [0.3036633  0.6963367 ]\n",
      " [0.95852894 0.04147113]\n",
      " [0.16282912 0.8371709 ]\n",
      " [0.95027006 0.0497299 ]\n",
      " [0.3036633  0.6963367 ]\n",
      " [0.95827836 0.04172162]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.747947   0.2520531 ]\n",
      " [0.05767478 0.9423252 ]\n",
      " [0.9574434  0.04255661]\n",
      " [0.0649257  0.9350743 ]\n",
      " [0.95841986 0.04158022]\n",
      " [0.32541698 0.67458296]\n",
      " [0.95658135 0.04341865]\n",
      " [0.12563619 0.87436384]\n",
      " [0.936903   0.06309696]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.7840702  0.21592979]\n",
      " [0.22554843 0.77445155]\n",
      " [0.9570681  0.04293185]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.85743886 0.14256117]\n",
      " [0.07301755 0.92698246]\n",
      " [0.936903   0.06309696]\n",
      " [0.0649257  0.9350743 ]\n",
      " [0.9550508  0.04494919]\n",
      " [0.41938943 0.58061063]\n",
      " [0.46918228 0.53081775]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.94757205 0.0524279 ]\n",
      " [0.22554843 0.77445155]\n",
      " [0.9573096  0.04269042]\n",
      " [0.34795046 0.65204954]\n",
      " [0.95852894 0.04147113]\n",
      " [0.05543154 0.9445685 ]\n",
      " [0.95827836 0.04172162]\n",
      " [0.06241911 0.93758094]\n",
      " [0.7840702  0.21592979]\n",
      " [0.05543154 0.9445685 ]\n",
      " [0.66462773 0.3353723 ]\n",
      " [0.05767478 0.9423252 ]\n",
      " [0.9550508  0.04494919]\n",
      " [0.22554843 0.77445155]\n",
      " [0.9574434  0.04255661]\n",
      " [0.16282912 0.8371709 ]\n",
      " [0.46918228 0.53081775]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.95682544 0.0431746 ]\n",
      " [0.05767478 0.9423252 ]\n",
      " [0.94473624 0.05526375]\n",
      " [0.26274168 0.7372583 ]\n",
      " [0.95841986 0.04158022]\n",
      " [0.06752574 0.9324743 ]\n",
      " [0.59416395 0.40583608]\n",
      " [0.05118935 0.9488107 ]\n",
      " [0.8006676  0.19933239]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.94757205 0.0524279 ]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.89060724 0.10939278]\n",
      " [0.06241911 0.93758094]\n",
      " [0.9168074  0.08319264]\n",
      " [0.07301755 0.92698246]\n",
      " [0.95682544 0.0431746 ]\n",
      " [0.05767478 0.9423252 ]\n",
      " [0.95754975 0.04245029]\n",
      " [0.14953493 0.85046506]\n",
      " [0.7840702  0.21592979]\n",
      " [0.06000306 0.93999696]\n",
      " [0.9309053  0.06909474]\n",
      " [0.08543883 0.91456115]\n",
      " [0.89060724 0.10939278]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.95682544 0.0431746 ]\n",
      " [0.14953493 0.85046506]\n",
      " [0.94113886 0.05886116]\n",
      " [0.07301755 0.92698246]\n",
      " [0.9087786  0.09122141]\n",
      " [0.12563619 0.87436384]\n",
      " [0.830943   0.16905698]\n",
      " [0.32541698 0.67458296]\n",
      " [0.94757205 0.0524279 ]\n",
      " [0.32541698 0.67458296]\n",
      " [0.89060724 0.10939278]\n",
      " [0.19224727 0.8077527 ]\n",
      " [0.95827836 0.04172162]\n",
      " [0.08964886 0.91035116]\n",
      " [0.830943   0.16905698]\n",
      " [0.22554843 0.77445155]\n",
      " [0.9574434  0.04255661]\n",
      " [0.20840824 0.79159176]\n",
      " [0.9577885  0.04221144]\n",
      " [0.05543154 0.9445685 ]\n",
      " [0.95218945 0.0478106 ]\n",
      " [0.06000306 0.93999696]\n",
      " [0.72844857 0.27155143]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.747947   0.2520531 ]\n",
      " [0.10540269 0.89459735]\n",
      " [0.95841986 0.04158022]\n",
      " [0.0649257  0.9350743 ]\n",
      " [0.9573096  0.04269042]\n",
      " [0.14953493 0.85046506]\n",
      " [0.9570681  0.04293185]\n",
      " [0.26274168 0.7372583 ]\n",
      " [0.85743886 0.14256117]\n",
      " [0.16282912 0.8371709 ]\n",
      " [0.95218945 0.0478106 ]\n",
      " [0.0532706  0.9467294 ]\n",
      " [0.8006676  0.19933239]\n",
      " [0.1770592  0.82294077]\n",
      " [0.95827836 0.04172162]\n",
      " [0.06000306 0.93999696]\n",
      " [0.9309053  0.06909474]\n",
      " [0.41938943 0.58061063]\n",
      " [0.9550508  0.04494919]\n",
      " [0.20840824 0.79159176]\n",
      " [0.86933655 0.13066345]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.86933655 0.13066345]\n",
      " [0.07301755 0.92698246]\n",
      " [0.9309053  0.06909474]\n",
      " [0.20840824 0.79159176]\n",
      " [0.94113886 0.05886116]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.88037986 0.11962017]\n",
      " [0.19224727 0.8077527 ]\n",
      " [0.830943   0.16905698]\n",
      " [0.12563619 0.87436384]\n",
      " [0.66462773 0.3353723 ]\n",
      " [0.05543154 0.9445685 ]\n",
      " [0.9580261  0.04197386]\n",
      " [0.07301755 0.92698246]\n",
      " [0.9582624  0.04173758]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.94473624 0.05526375]\n",
      " [0.12563619 0.87436384]\n",
      " [0.9309053  0.06909474]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.6182513  0.38174874]\n",
      " [0.34795046 0.65204954]\n",
      " [0.9087786  0.09122141]\n",
      " [0.06752574 0.9324743 ]\n",
      " [0.4441456  0.55585444]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.95682544 0.0431746 ]\n",
      " [0.06000306 0.93999696]\n",
      " [0.95754975 0.04245029]\n",
      " [0.07022202 0.92977804]\n",
      " [0.936903   0.06309696]\n",
      " [0.05543154 0.9445685 ]\n",
      " [0.86933655 0.13066345]\n",
      " [0.0759153  0.9240847 ]\n",
      " [0.95841986 0.04158022]\n",
      " [0.10540269 0.89459735]\n",
      " [0.9577885  0.04221144]\n",
      " [0.08202943 0.9179706 ]\n",
      " [0.51959646 0.48040354]\n",
      " [0.2827542  0.71724576]\n",
      " [0.641771   0.35822898]\n",
      " [0.3036633  0.6963367 ]\n",
      " [0.6182513  0.38174874]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.51959646 0.48040354]\n",
      " [0.13714825 0.8628517 ]\n",
      " [0.46918228 0.53081775]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.9168074  0.08319265]\n",
      " [0.3036633  0.6963367 ]\n",
      " [0.59416395 0.40583608]\n",
      " [0.12563619 0.87436384]\n",
      " [0.95754975 0.04245029]\n",
      " [0.2827542  0.71724576]\n",
      " [0.8006676  0.19933239]\n",
      " [0.32541698 0.67458296]\n",
      " [0.4441456  0.55585444]\n",
      " [0.05118935 0.9488107 ]\n",
      " [0.4441456  0.55585444]\n",
      " [0.1770592  0.82294077]\n",
      " [0.89060724 0.10939278]\n",
      " [0.16282912 0.8371709 ]\n",
      " [0.95218945 0.0478106 ]\n",
      " [0.12563619 0.87436384]\n",
      " [0.72844857 0.27155143]\n",
      " [0.06752574 0.9324743 ]\n",
      " [0.95841986 0.04158022]\n",
      " [0.0532706  0.9467294 ]\n",
      " [0.9309053  0.06909474]\n",
      " [0.26274168 0.7372583 ]\n",
      " [0.9309053  0.06909474]\n",
      " [0.04918518 0.9508149 ]\n",
      " [0.936903   0.06309696]\n",
      " [0.39503235 0.60496765]\n",
      " [0.86933655 0.13066345]\n",
      " [0.08202943 0.9179706 ]\n",
      " [0.9168074  0.08319265]\n",
      " [0.0649257  0.9350743 ]\n",
      " [0.94473624 0.05526375]\n",
      " [0.2827542  0.71724576]\n",
      " [0.9309053  0.06909474]\n",
      " [0.1770592  0.82294077]\n",
      " [0.9584744  0.04152564]\n",
      " [0.08964885 0.91035116]\n",
      " [0.49437502 0.50562495]\n",
      " [0.06241911 0.93758094]\n",
      " [0.95631385 0.04368616]\n",
      " [0.14953493 0.85046506]\n",
      " [0.86933655 0.13066345]\n",
      " [0.0532706  0.9467294 ]\n",
      " [0.95852894 0.04147113]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.641771   0.35822898]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.9087786  0.09122141]\n",
      " [0.2827542  0.71724576]\n",
      " [0.641771   0.35822898]\n",
      " [0.34795046 0.65204954]\n",
      " [0.94473624 0.05526375]\n",
      " [0.12563619 0.87436384]\n",
      " [0.95218945 0.0478106 ]\n",
      " [0.20840824 0.79159176]\n",
      " [0.72844857 0.27155143]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.8162882  0.18371187]\n",
      " [0.16282912 0.8371709 ]\n",
      " [0.88037986 0.11962017]\n",
      " [0.04918518 0.9508149 ]\n",
      " [0.747947   0.2520531 ]\n",
      " [0.1770592  0.82294077]\n",
      " [0.94473624 0.05526375]\n",
      " [0.19224727 0.8077527 ]\n",
      " [0.95658135 0.04341865]\n",
      " [0.08543883 0.91456115]\n",
      " [0.95658135 0.04341865]\n",
      " [0.12563619 0.87436384]\n",
      " [0.9309053  0.06909474]\n",
      " [0.32541698 0.67458296]\n",
      " [0.86933655 0.13066345]\n",
      " [0.22554843 0.77445155]\n",
      " [0.95027006 0.0497299 ]\n",
      " [0.12563619 0.87436384]\n",
      " [0.9573096  0.04269042]\n",
      " [0.41938943 0.58061063]\n",
      " [0.9583651  0.04163487]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.49437502 0.50562495]\n",
      " [0.32541698 0.67458296]\n",
      " [0.59416395 0.40583608]\n",
      " [0.26274168 0.7372583 ]\n",
      " [0.8006676  0.19933239]\n",
      " [0.10540269 0.89459735]\n",
      " [0.8006676  0.19933239]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.9087786  0.09122141]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.95827836 0.04172162]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.8006676  0.19933239]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.85743886 0.14256117]\n",
      " [0.34795046 0.65204954]\n",
      " [0.70803076 0.29196927]\n",
      " [0.08202943 0.9179706 ]\n",
      " [0.8006676  0.19933239]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.9583651  0.04163487]\n",
      " [0.07022202 0.92977804]\n",
      " [0.9577885  0.04221144]\n",
      " [0.10540269 0.89459735]\n",
      " [0.94473624 0.05526375]\n",
      " [0.08964886 0.91035116]\n",
      " [0.95682544 0.0431746 ]\n",
      " [0.37118566 0.6288143 ]\n",
      " [0.86933655 0.13066345]\n",
      " [0.07022202 0.92977804]\n",
      " [0.56961465 0.43038532]\n",
      " [0.1770592  0.82294077]\n",
      " [0.7840702  0.21592979]\n",
      " [0.07301755 0.92698246]\n",
      " [0.7840702  0.21592979]\n",
      " [0.26274168 0.7372583 ]\n",
      " [0.95027006 0.0497299 ]\n",
      " [0.0532706  0.9467294 ]\n",
      " [0.9309053  0.06909474]\n",
      " [0.07022202 0.92977804]\n",
      " [0.95841986 0.04158022]\n",
      " [0.2827542  0.71724576]\n",
      " [0.95027006 0.0497299 ]\n",
      " [0.12563619 0.87436384]\n",
      " [0.766494   0.2335061 ]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.9580261  0.04197386]\n",
      " [0.12563619 0.87436384]\n",
      " [0.86933655 0.13066345]\n",
      " [0.3036633  0.6963367 ]\n",
      " [0.7840702  0.21592979]\n",
      " [0.06241911 0.93758094]\n",
      " [0.9584744  0.04152564]\n",
      " [0.07301755 0.92698246]\n",
      " [0.9574434  0.04255661]\n",
      " [0.16282912 0.8371709 ]\n",
      " [0.9583651  0.04163487]\n",
      " [0.09644442 0.9035556 ]\n",
      " [0.51959646 0.48040354]\n",
      " [0.32541698 0.67458296]\n",
      " [0.9582624  0.04173758]\n",
      " [0.07591529 0.9240847 ]\n",
      " [0.9582624  0.04173758]\n",
      " [0.32541698 0.67458296]\n",
      " [0.59416395 0.40583608]\n",
      " [0.0649257  0.9350743 ]\n",
      " [0.9583651  0.04163487]\n",
      " [0.13714826 0.8628517 ]\n",
      " [0.89060724 0.10939278]\n",
      " [0.19224727 0.8077527 ]\n",
      " [0.936903   0.06309696]\n",
      " [0.06752574 0.9324743 ]\n",
      " [0.9573096  0.04269042]\n",
      " [0.07891826 0.9210817 ]\n",
      " [0.641771   0.35822898]\n",
      " [0.1770592  0.82294077]\n",
      " [0.95754975 0.04245029]\n",
      " [0.11510891 0.8848911 ]\n",
      " [0.92418844 0.07581156]\n",
      " [0.22554843 0.77445155]\n",
      " [0.9573096  0.04269042]\n",
      " [0.39503235 0.60496765]\n",
      " [0.9577885  0.04221144]\n",
      " [0.24366431 0.7563357 ]\n",
      " [0.95754975 0.04245029]\n",
      " [0.04918518 0.9508149 ]\n",
      " [0.95841986 0.04158022]\n",
      " [0.34795046 0.65204954]\n",
      " [0.85743886 0.14256117]\n",
      " [0.08543883 0.91456115]\n",
      " [0.9574434  0.04255661]\n",
      " [0.05543154 0.9445685 ]\n",
      " [0.4441456  0.55585444]\n",
      " [0.2827542  0.71724576]\n",
      " [0.6182513  0.38174874]\n",
      " [0.34795046 0.65204954]\n",
      " [0.56961465 0.43038532]\n",
      " [0.08964885 0.91035116]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape) # notice for each sample two columns\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420,)\n",
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(rounded_predictions.shape) # notice the shape difference, 0 if first val big, 1 if second big\n",
    "print(rounded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[190  20]\n",
      " [ 10 200]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_plot_labels = ['no_side_effects','had_side_effects']\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVIEWED ON 24 August 2021 -----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question ?\n",
    "\n",
    "1. Batch size and epoch?\n",
    "\n",
    "Suppose there are 1000 training data, one round of training with 1000 is called 1 epoch\n",
    "during this 1 epoch, we can provide the 1000 data as different batches\n",
    "Say batchsize=10, ie, 100 batches will complete the epoch\n",
    "\n",
    "Why as batches instead of 1 by 1 :\n",
    "\n",
    "Because, based on the computing power, it may be possible to training with multiple inputs at a time,\n",
    "But quality may degrade, even if machine can handle large batch sizes\n",
    "But if we give large batch size, machine maynot have that much power,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVIEWED ON 24th August 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
